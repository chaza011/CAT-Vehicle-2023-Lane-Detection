import torch
import torch.nn as nn
import torchvision
from torch.utils.data import DataLoader
import torch.optim as optim

#Hyperparameters
batch_size = 32
epochs = 5
lr = 1e-3
pos_weight = 5 #Weight for loss function lane class. We utilize a weighted loss function because the lane class has less examples than the non-lane class

#Data
#Train
X_train = torch.load('data/path')
y_train = torch.load('data/path')
train_Data = torch.utils.data.TensorDataset(X_train, y_train)
#Test
X_test = torch.load('data/path')
y_test = torch.load('data/path')
test_Data = torch.utils.data.TensorDataset(X_test, y_test)

#Loaders
train_loader = DataLoader(train_Data, batch_size = batch_size, shuffle=True, drop_last=False)
test_loader = DataLoader(test_Data, batch_size = batch_size, shuffle=True, drop_last=False)

#Initialize model
model = model #Import your model your model or define it 

#Optimizer
opt = optim.Adam(model.parameters(), lr = lr)

#Loss Function
def weighted_BCE(y_pred, y_true, pos_weight):
    #Flatten
    y_pred_flat = y_pred.view(-1)
    y_true_flat = y_true.view(-1)
    # Define the binary cross-entropy loss function with logits (the sigmoid activation function is applied here)
    bce_logits_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

    # Calculate the loss
    loss = bce_logits_loss(y_pred_flat, y_true_flat)

    return loss

#F1-Score
def f1_score(y_pred, y_true, epsilon=1e-7):
    # Flatten the tensors
    y_pred_flat = y_pred.view(-1)
    y_true_flat = y_true.view(-1)

    # Convert to binary values (0 or 1) based on a threshold (e.g., 0.5)
    y_pred_flat = (y_pred_flat > 0.5).float()
    #y_true_flat = (y_true_flat > 0.5).float()

    # True Positives, False Positives, False Negatives
    tp = torch.sum((y_true_flat == 1) & (y_pred_flat == 1)).float()
    fp = torch.sum((y_true_flat == 0) & (y_pred_flat == 1)).float()
    fn = torch.sum((y_true_flat == 1) & (y_pred_flat == 0)).float()

    # Calculate precision and recall
    precision = tp / (tp + fp + epsilon)
    recall = tp / (tp + fn + epsilon)

    # Calculate F1 score
    f1 = 2 * (precision * recall) / (precision + recall + epsilon)

    return f1

#Training Epoch
def epoch(model, loader, opt=None):
  #i = 0
  total_loss = 0.
  total_F1 = 0.
  #Load data
  for X,y in loader:
    X,y = X.to(device), (y).to(device)
    #Get model output
    yp = model(X)
    #Get loss
    loss = weighted_BCE(yp,y,torch.tensor(pos_weight))
    F1 = f1_score(yp, y)
    total_loss += loss.item() * X.shape[0]
    total_F1 += F1.item() * X.shape[0]
    #Optimize
    if opt:
      opt.zero_grad()
      loss.backward()
      opt.step()
  #End Epoch
  return total_loss / (len(loader.dataset)), total_F1 / (len(loader.dataset))

#Training
for i in range(epochs):
  print(f'Epoch: {i+1}')
  print('---------------------')
  #Train
  print("Train")
  train_loss, train_F1 = epoch(model, train_loader, opt)
  print('Train Loss: {train_loss}')
  print('Train F1: {train_f1}')
  print('---------------------')
  #Small Test
  print("Test")
  test_loss,test_F1 = epoch(model, test_loader)
  print('Test Loss: {test_loss}')
  print('---------------------')
  print('Test F1: {test_f1}')
